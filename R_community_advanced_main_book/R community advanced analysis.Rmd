--- 
title: "R community advanced analysis"
author: "Matthew R. Gemmell"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
favicon: figures/NEOF_favicon.png
description: NEOF book for the R community advanced analysis course
cover-image: "figures/NEOF.png"
---
```{r include=FALSE, cache=FALSE}
library(webexercises)
```

```{r, echo=FALSE}
#Change colour, border, and text of code chunks
#Check style.css for .Rchunk
#https://stackoverflow.com/questions/65627531/change-r-chunk-background-color-in-bookdown-gitbook
#https://bookdown.org/yihui/rmarkdown-cookbook/chunk-styling.html
knitr::opts_chunk$set(class.source="Rchunk") 
```

```{r cite-packages, include = FALSE}
# automatically create a bib database for R packages
# add any packages you want to cite here
knitr::write_bib(c(
  .packages(), 'bookdown', 'webexercises'
), 'packages.bib')
```

<center>
![](figures/NEOF.png){style="border-radius: 15px; width: 300px"}
</center>

# (PART\*) Intro {-}

# Introduction

ADD NEW COURSE LOGO

ADD NEW COURSE INTRO

## Table of contents {-}

ADD TABLE OF CONTENTS

<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.

<!--chapter:end:01-R_community_advanced_main_book.Rmd-->

```{r include=FALSE, cache=FALSE}
library(webexercises)
```
# Dataset & workflow {#datasetandworkflowchap}
<center>
![](figures/data.png){style="width:200px"}
</center>

## Dataset
<center>
![](figures/freshwater_france.png){style="border-radius: 5px; width: 500px; border: 5px solid #333333"}
</center>

In this tutorial we will be using a 16S metabarcoding dataset derived from surface water from the Durance River in the south-east of France.
Two major comparisons were carried out in combination with each other.

[Link to paper](https://www.mdpi.com/2076-2607/8/8/1129)

### Sites
<center>
![](figures/river.png){style="width:200px"}
</center>

Three different sites were chosen on the Durance River. These three sites were representative of an anthropisation (transformation of land by humans) gradient along a river stream. These sites were:

- __Upper Durance sampling site (UD)__: Alpine part of the river with little/no anthropisation.
- __Middle Durance sampling site (MD)__: Upper part of agricultural land dominated by apple and pear production.
- __Lower Durance sampling site (LD)__: Lower part of agricultural land with intensive production of fruits, cereals, and vegetables.

### Culture media
<center>
![](figures/petri.png){style="width:200px"}
</center>

Surface water was sampled and different culture media were used to produce bacterial lawns for each site. The media used were:

- __Environmental sample (ENV)__: No media used, frozen at -20°C.
- __TSA 10%__ incubated at 28°C for 2 days.
- __KBC__ incubated at 28°C for 2 days.
- __CVP__ incubated at 28°C for 3 days.

### Summary & questions {#sum_and_qs}
<center>
![](figures/sum_red.png){style="width:200px"}
</center>

Each sample and media combination was produced in replicates of three giving a total of 36 samples (3 X 4 X 3 = 36). The three replicates were cultured on three different plates with the same media. An ASV table, taxonomy table, and phylogenetic tree were produced with QIIME2 and DADA2.

With this data we can ask and investigate the following questions:

- How do the bacterial communities change across the anthropisation gradient?
- Is there a difference in the replicates of one site and media combination? I.e. do any of the media produce inconsistent profiles?
- Is there more difference between the sites or the media used?
- Do the media samples differ from the ENV samples? If so, how?

## Workflow
<center>
![](figures/workflow.png){style="width:200px; border-radius:15px; background:white"}
</center>

1. [Import](#import_chap): Import QIIME2 artifacts into a `phyloseq` object with `qiime2R`.
2. [Summarisations](#sum_phyloseq_chap): Check our `phyloseq` object with summarisations.
3. [Minimum depth](#mindepthchap): Determine the minimum depth we should use and remove samples with lower depth.
4. [Taxanomic relative abundance](#taxa_relabund_chap): Create taxonomic relative abundance tables.
5. [Taxa plots](#chaptaxaplots): Produce heat maps and bar plots of taxa relative abundances.
6. [Family and genus](#family_genus_chap): Using the last step to produce family and genus based taxa plots.
7. [Rarefaction](#rarefaction_chap): Carry out sample depth normalisation with rarefactions. This will be used for alpha and beta diversity analysis.
8. [Alpha diversity](#alpha_chap): Carry out alpha diversity analysis through plots and statistics.
9. [Beta diversity](#beta_chap): Carry out beta diversity analysis through plots and statistics.
10. [Differenital abundance anlaysis](#DA_chap): Detect biomarkers compared to a reference group with ANCOM.

<!--chapter:end:02-Dataset_and_workflow.Rmd-->

```{r include=FALSE, cache=FALSE}
library(webexercises)
```
# R Packages {#rpackchap}
<center>
![](figures/R.png){style="width:200px"}
</center>

During this workshop we will use various R packages with their own intricacies. Before going into analysis we'll introduce you to some of these important concepts.

## R packages/libraries
<center>
![](figures/r_package.png){style="width:200px"}
</center>

R packages/libraries contain additional functions, data and code for analysing, manipulating and plotting different types of data. Many common packages will be installed as default when you install R. Other more specialised packages, such as the `ggplot2` package, must be installed by the user.

Packages found on The Comprehensive R Archive Network (CRAN), R’s central software repository, can be installed using the following command.

```{r eval=FALSE}
install.packages("package_name")
```

Every time you reload R you will need to load the packages you require if they are not installed in R by default. To do this type:

```{r eval=FALSE}
library("package_name")
```

I generally have a list of `library()` functions at the top of my R scripts (`.R` files) for all the packages I use in the script.

Throughout this course you will get a lot of practice installing and loading various packages.

`r hide("R package or R Library?")`
R packages are a collection of R functions, data, and compiled code. You can install these into a directory on your computer.

An R library is a directory containing a R package.

Because of this, the terms R package and R library may be used synonymously. We will use the term package in this workshop.
`r unhide()`

As we will be using a lot of packages we shall use double colons (`::`) to specify which package each function belongs to, unless the function is from base R. For example if we use the function `summarize_phyloseq()` from the package `microbiome` we would type the function like below:

__Note__: Do not run the below command.

```{r eval=FALSE}
microbiome::summarize_phyloseq()
```

This convention has 2 benefits:

- We can easily tell which R package each function comes from.
  - This is useful for your future coding where you may copy some, but not all, commands from one script to another. You will therefore know which packages you will need to load.
  - If you need some more documentation about a function you will know what package to look up.
  - Writing your methods will be a lot easier.
- Different packages may have functions with the same name. Specifying the package will ensure you are using the correct function.

## The grammar of graphics
<center>
![](figures/ggplot2.png){style="width:200px"}
</center>

During this course we will be using the grammar of graphics coding approach. This approach is implemented by the R package `ggplot2` to create visualisations such as bar charts, box plots, ordination plots etc. In turn `ggplot2` is used by a host of other packages, some of which we will be using. Although `ggplot2` is R code, its structure is very different and it takes effort to learn. Thankfully, `ggplot2` is very powerful and flexible, and it produces very professional and clean plots.

We will use the `iris` dataset (inbuilt into R) to show an example of `ggplot2` code and its visualisation output. You don't need to run the below code.

__Note__: If you would like to see the contents of the `iris` dataset you can run the command `View(iris)` in your R instance later.

```{r, fig.align='center', f}
#Load library
library(ggplot2)

#Create new ggplot2 object using iris dataset
ggplot2::ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width, colour=Species)) +
  #Make the object a scatter plot 
  ggplot2::geom_point() +
  #Add plot tile
  ggplot2::ggtitle("Iris Sepal length vs width") +
  #Set x and y axis label names
  ggplot2::labs(x = "Sepal length", y = "Sepal width")
```

We will not learn `ggplot2` specifically during this course. However, the structure of creating an object will be used. In the above case the initial object was built with `ggplot`. Subsequently additions and edits were carried out with `+` and various other functions.

An important concept of the grammar of graphics is aesthetics. Aesthetics are the parts of a graphic/plot. In the above command we set the aesthetics with the function `aes()` within the `ggplot()` function. The X aesthetic (i.e. what values are assigned to the x axis) was set as the Sepal length values from the column `Sepal.Length` of the dataframe `iris`. In turn the Y axis values are set to the Sepal width and the colouring of the points are set to the Species.

That was a quick introduction to the grammar of graphics. We will be using this to create visualisations with a `phyloseq` object using various R packages specifically designed for community abundance data within `phyloseq` objects.

For more resources on `ggplot2` please see the [appendix](#ggplot2_appendix) of this book.

## phyloseq
<center>
![](figures/phyloseq_logo.png){style="width:200px"}
</center>

In this book we will be working with [`phyloseq`](https://joey711.github.io/phyloseq/) objects to preprocess  our dataset, create visualisations, and carry out statistical analyses. This is a very popular object type for community abundance datasets as it contains the abundance table, metadata, and taxonomy table in one object, optionally containing the phylogenetic tree and reference sequences if wanted/required.

<center>
![](figures/phyloseq_input.png){style="border-radius: 15px; width: 800px; border: 5px solid #333333"}
</center>

For more info on `phyloseq` and associated packages please see the [appendix](#phyloseq_appendix).

<!--chapter:end:03-R_packages.Rmd-->

```{r include=FALSE, cache=FALSE}
library(webexercises)
```
# Set-up {#setupchap}
<center>
![](figures/start.png){style="width:200px"}
</center>

Prior to any analysis we need to setup our environment in the webVNC.

## Logon instructions {#cluster}

For this workshop we will be using Virtual Network Computing (VNC). Connect to the VNC with a browser by using the webVNC link you were sent.

You will now be in a logged-in Linux VNC desktop. 
You will see something as below (there may be only one terminal which is fine). 
If you do not see something similar please ask for assistance.

<center>
![](figures/logon_pic.png){style="width:800px"}
</center>

If the VNC is taking up too much/little space of your browser you can use the zoom of your browser to adjust the size. 
You will most likely need to use your browser's tool bar to accomplish this. 
Ensure you can see the grey borders.

These instructions will not work outside of this workshop. 
If you would like to install your own Linux OS on your desktop or laptop we would recommend Mint Linux 

The following link is a guide to install Mint Linux:  
https://linuxmint-installation-guide.readthedocs.io/en/latest/

## Mamba
<center>
![](figures/mamba_logo.png){style="width:200px; border-radius:15px; background:null"}
</center>

This workshop requires a lot of packages. 
These all can be difficult to install with R. 
Instead we have used Mamba forge to install R, its packages, and Jupyter-notebook (more info below). 
To learn more about Mamba-forge and how to create your own environment please see the [appendix](#mamba_install).

To set-up your environment for this workshop please run the following code (you must include the full stop and space at the front of the command).

```{bash, eval=FALSE}
. usercommunity
```

You will have successfully activated the environment if you now see `(r_community)` at the start of your command prompt. 
This indicates you are now in the mamba environment called `r_community` created by the instructor.

If you are interested in the use script you can look at its contents.

```{bash, eval=FALSE}
less /usr/local/bin/usercommunity
```

__Tip:__ press `q` to quit `less`.

For more about mamba and how to create your own `r_community` environment please see the [appendix](#mamba_install)

<!--chapter:end:04-Setup.Rmd-->

```{r include=FALSE, cache=FALSE}
library(webexercises)
```
# Jupyter {#jupyterchap}

<center>
![](figures/jupyter_logo.png){style="border-radius: 15px; width: 200px; background-color: white"}
</center>


[`Jupyter-notebook`](https://jupyter.org/) is a nice browser based method to write, edit, and run code. It was initally created for Python coding, but has since branched out to many other languages, such as `R`.

We are using it in this workshop for a variety of its properties:

-   It is popular and well maintained.
-   It is lightweight. Other heavier weight programs, such as RStudio, would struggle in our HPC due to the graphical and CPU load.
-   It is interactive and displays code output.
-   It allows for easier annotation, editing, and debugging than the command line.
-   It provides a graphical interface for changing directories and choosing files.

Before carrying out any analysis we will go through a quick tutorial of `jupyter-notebook`.

__Note:__ There is a [video tutorial](#jup_vid_tut) of this chapter at the bottom of this page if you prefer to watch it.

## Open Jupyter-notebook

The first step is to open `jupyter-notebook`. Run the below command in your `(r_community)` environment.

```{bash, eval=FALSE}
jupyter-notebook
```

This will open `jupyter-notebook` in firefox. We won't need to access the linux terminal anymore. Leave the terminal running `jupyter-notebook` and full screen your `firefox` so you should see something like below.

`r hide("Can't see the whole webVNC window?")`
You may need to zoom out with your browser so you can see the full webVNC window.

- __Chrome__: Click on the __three dots in vertical line__ (
```{r, echo=FALSE}
icons::ionicons("ellipsis-vertical")
``` 
) on the top left for a dropdown menu which includes zoom options.
- __Edge__: Click on the __three horizontal lines__ (
```{r, echo=FALSE}
icons::ionicons("ellipsis-horizontal")
``` 
) on the top left for a dropdown menu which includes zoom options.
- __Firefox__: Click on the __three dots in horizontal line__ (
```{r, echo=FALSE}
icons::fontawesome("bars")
```
) on the top left for a dropdown menu which includes zoom options.
`r unhide()`

<center>
![](figures/jupyter_notebook_example_1.png){style="width: 1000px; border-radius: 15px; border: 5px solid #333333"}
</center>

## Create R notebook

The next step is to create an R notebook.

1.  Click on the **"New"** button towards the top right, right of the "Upload" button.
2.  From the dropdown click **"R"** below "Python 3 (ipykernel)".

This will open up a new R notebook like below.

<center>
![](figures/jupyter_notebook_example_2.png){style="width: 1000px; border-radius: 15px; border: 5px solid #333333"}
</center>

## Cells and code

`Jupyter-notebook` uses **cells** (the grey boxes) to separate code. This is very useful to compartmentalise our code.

There will already be one **cell**. Within the **cell**, type in the below commands.

```{r, eval=FALSE}
1+1
2-3
```

When pressing enter in **cells** it will create a new line. To run all commands in a **cell** press `CTRL + enter`.

Run your current **cell** and you should see something like below.

<center>
![](figures/jupyter_notebook_example_3.png){style="width: 1000px; border-radius: 15px; border: 5px solid #333333"}
</center>

## Create new cells

You can create new **cells** by 2 different means.

-   Press the `+` button on the tool bar (between the floppy disk
```{r, echo=FALSE}
icons::icon_style(icons::ionicons("save-outline"), background="white")
``` 
and scissors
```{r, echo=FALSE}
icons::icon_style(icons::ionicons("cut"), background="white")
``` 
). This will add a **cell** below your currently selected **cell**.
-   Click on the **`Insert`** button and use the dropdown to add a cell above or below your currently selected cell.

**Tip:** Hover over the toolbar icons to display a text based description of its function.

With that knowledge add a second **cell** below the first **cell**. Add the following code to your second **cell** but do not run it.

```{r, eval=FALSE}
num_1 <- 3
num_2 <- 10
```

**Tip:** Notice there are green lines around your selected cell.

Insert a third **cell** and add the following code to it. Do not run the code.

```{r, eval=FALSE}
num_1 * num_2
```

## Running code

Try to run the code in the third **cell**. There should be an error as we have not created the objects `num_1` & `num_2`. We have only written the code for these objects but not run them.

We can run all the code in a notebook starting from the first **cell** to the last **cell**.

To run all **cells** from the start:

-   Click on the **"Cell"** button.
-   Click **"Run All"** from the drop-down options.

You should then see something like the below in your notebook.

<center>
![](figures/jupyter_notebook_example_4.png){style="width: 1000px; border-radius: 15px; border: 5px solid #333333"}
</center>

There is no output printed for __cell__ 2 because we are assigning variables. However, the correct output for Cell 3 is below it. This is because the variables were assigned in **cell** 2 before **cell** 3 was run.

## Saving the file
<center>
![](figures/r_save.png){style="width:100px"}
</center>

As with RStudio and other good coding interfaces we can save our notebook.

First we should rename the file. Rename the notebook to **"jupyter_tut.ipynb"**:

1.  Click on the name of the notebook, currently called **"Untitled"**.
    -   This is at the very top of the notebook, right of the Jupyter logo.
2.  A pop-up called **"Rename Notebook"** will appear. Change the Name to **"jupyter_tut.ipynb"**.
3.  Click **"Rename"**.

Now we can save the file. Two methods to save are:

-   Click the floppy disk 
```{r, echo=FALSE}
icons::icon_style(icons::ionicons("save-outline"), background="white")
``` 
on the toolbar.
-   Click on the **"File"** button. Click **"Save and Checkpoint"** from the dropdown options.

## Title cells with markdown

We will be using multiple notebooks in this workshop. We will also have multiple sections per notebook. It will be useful to create header cells with markdown to create visual separation of the different sections.

To add a header __cell__ to the top of our notebook:

-   Create a new __cell__ at the top of the notebook.
-   Click on the **"Code"** drop down and select **"Markdown"**.
    -   The **"Heading"** option no longer works.

<center>![](figures/jupyter_notebook_example_5.png){style="width: 1000px; border-radius: 15px; border: 5px solid #333333"}</center>

-   Add the following to the **"Markdown"** cell to create a first level header.
    -   Ensure you have a space between the `#` and header text ("Tutorial").

```{r, eval=FALSE}
# Tutorial
```

Great, we can now add nice headers in our notebooks. **Save** the notebook once more before carrying on to the next section.

`r hide("Markdown")` 
You won't need to know more about `Markdown` but if you are interested please see the [`Markdown` guide](https://www.markdownguide.org/basic-syntax/). 
`r unhide()`

## Close the notebook

To close the notebook:

-   Click on **"File"**.
-   From the dropdown options click **"Close and Halt"**.

When you are back in the file explorer page you may not yet see the new file you saved. If so, you will need to refresh the page with the Refresh button 
```{r, echo=FALSE}
icons::icon_style(icons::ionicons("refresh"), background="white")
``` 
towards the top right.

<center>
![](figures/jupyter_notebook_refresh.png){style="width: 1000px; border-radius: 15px; border: 5px solid #333333"}
</center>

With that quick tutorial of `jupyter-notebook` we can start our community analysis in the next chapter.

For more info on `jupter-notebook` please see the [appendix](#jupyter_appendix).

## Video tutorial {#jup_vid_tut}
<div class="container">
<iframe src="https://www.youtube.com/embed/-c_6HoPMw9g" 
frameborder="0" allowfullscreen class="video"></iframe>
</div>

<!--chapter:end:05-Jupyter.Rmd-->

```{r include=FALSE, cache=FALSE}
library(webexercises)
```
# (PART\*) Iterative rarefaction {.unnumbered}

# Iterative rarefaction intro {#rarefaction_intro_chap}

<center>
![](figures/iteration.png){style="width:200px"}
</center>

__Rarefaction__ is the process of randomly subsetting samples so the total count values are identical across all samples.
__Rarefaction__ is intended to correct for bias caused by varying sampling depths across the samples to be analysed.

## Should you rarefy?
<center>
![](figures/choice_sign.png){style="width:200px"}
</center>

__Rarefaction__ can be a hotly debated topic with two main points of view.

1.  Some researchers believe it is not appropriate.
    1.  This is backed up by the 2014 paper ["Waste Not, Want Not: Why Rarefying Microbiome Data Is Inadmissible"](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003531)
    2.  Various R package developers do not recommend it such as the developers of [`phyloseq`](https://joey711.github.io/phyloseq/) & [`microbiome`](https://microbiome.github.io/tutorials/).
2.  Some researchers believe it is appropriate.
    1.  This is backed up by the 2022 paper ["To rarefy or not to rarefy: robustness and efficiency trade-offs of rarefying microbiome data"](https://academic.oup.com/bioinformatics/article/38/9/2389/6536959)
    2.  The QIIME2 developers include rarefaction in their tutorials/SOPs for alpha and beta diversity analysis

We use __rarefaction__ in our analyses but it is ultimately up to you whether you utilise it or not.

## Using iterations
<center>
![](figures/iterations.png){style="width:200px; background-color:white; border-radius:15px"}
</center>

In our initial R community analysis workshop we only __rarefy__ once for each sample.
In this section we will __rarefy__ multiple times to calculate average values for alpha and beta diversity metrics.
This is __iterative rarefaction__ analysis.

This iterative approach will, in theory, smooth out any extreme results one round of __rarefaction__ may cause. Extreme results are possibly due the random nature of __rarefaction__.
These extreme results can include:

-   Leaving important features (ASVs, taxonomic groups, etc.) with no counts
-   Causing a few features to have much higher relative abundances
-   Varying alpha and beta diversity values with different sets of rarefaction

## Section contents
<center>
![](figures/contents.png){style="width:200px; background-color:white; border-radius:15px"}
</center>

In this section we will learn how to:

- Use __random seeds__ for sampling
- Carry out __iterative rarefaction__ with sets of __random seeds__
- Use __iterative rarefaction__ to carry out alpha diversity analysis
- Use __iterative rarefaction__ to carry out beta diversity analysis

<!--chapter:end:06-Iterative_rarefaction.Rmd-->

```{r include=FALSE, cache=FALSE}
library(webexercises)
```
# Random seeds {#random_seeds_chap}
<center>
![](figures/random_seeds.png){style="width:200px"}
</center>

What are __random seeds__?

__Random seeds__ are numbers that computational tasks use to determine how they will carry out a random task.

In this chapter we will demonstrate the use of __random seeds__. 
This is to help understand what they are and why they are used.
We won't do anything of value with the results in this chapter, instead this knowledge will be useful for understanding __iterative rarefaction__.

## Random seed notebook

<center>
![](figures/seed_notebook.png){style="width:100px"}
</center>

Create a new R jupyter-notebook called "Random_seeds.ipynb".
We will use this for this chapter.

## Random sampling
<center>
![](figures/random_sampling.png){style="width:200px; background:white; border-radius:5px"}
</center>

To demonstrate to use of random seed we will use the R function `sample()`. 
This function randomly samples a set of numbers.

Create the below code in a code cell.

```{R, eval=FALSE}
#Create a vector containing the numbers 0 to 10
num_vec <- 0:10
#Randomly sample 5 of these numbers
sample(x = num_vec, size = 5)
```

If you run the code you will get five random single digit numbers.

Run this multiple times and you will hopefully see the sampled numbers are different every time.

## Sampling with replacement
<center>
![](figures/balls.png){style="width:200px"}
</center>

You may also notice that within each sample there are no repeating numbers.
You can change this by adding the options `replace = TRUE`.

Try this out in a new cell.

```{R, eval=FALSE}
#Randomly sample 5 of these numbers with replacement
sample(x = num_vec, size = 5, replace = TRUE)
```

Run this a few times and you will hopefully notice that the five numbers are not always unique.

When sampling __with replacement__ you put back any results back into the sampling pool.
When sampling __without replacement__ you don't put back any results into the sampling pool.

The famous example is sampling green and yellow balls from a bag.
If you had a bag with 1000 balls and you wanted a rough idea of the ratio of yellow and green balls you could count the number of these balls within a sample of only 50.
Without replacement you would take out a ball, record its colour and throw it in a separate container.
With replacement you would take out a ball, record its colour and put it back into the initial bag, meaning it could possibly be recounted.

One advantage of sampling with replacement is that your sampling size can be larger than your actual population size.
For example, you could create a random sampling of 50 with a bag containing 10 balls with replacement.
This would not work without replacement.
The below script will cause R to produce an error saying it can be done with replacement.

```{R, eval=FALSE}
#Randomly sample 5 of these numbers with replacement
sample(x = num_vec, size = 5, replace = TRUE)
```

Importantly for us __rarefaction__ uses sampling __without replacement__.

## Setting a random seed
<center>
![](figures/set_a_seed.png){style="width:400px"}
</center>

We'll sample, without replacement, the numbers one more time in a new cell.
However, this time we will set the __random seed__ with the function `seet.seed()`.

```{R, eval=FALSE}
#Set random seed
set.seed(1234)
#Randomly sample 12 of these numbers without replacement
sample(x = num_vec, size = 12, replace = FALSE)
#Reset random seed
set.seed(NULL)
```

Before we explain the code further, try running the cell multiple times.
If it is identical to the above code you will get the numbers "9, 5, 4, 3, & 6".

How come you are getting these results if it is random?

True randomness is pretty much impossible, especially in computing.
Therefore, many programs use __seeds__ to determine how random tasks will be carried out.
Various programs that use __random seeds__ include:

- Sampling tools such as `sample()` and rarefaction
- Creating bootstrapped phylogenies
- Creating procedural content such as building Minecraft worlds

If you run a tool that uses random sampling you will always get the same results if:

- You use the same random seed
- You use the same data
- You use the same parameters including the replacement method (with or without)

In fact, run the below code in a new code cell and you may notice a similarity with your previous output.

```{R, eval=FALSE}
#Set random seed
set.seed(1234)
#Create a vector containing the numbers 0 to 10
larger_num_vec <- 10:19
#Randomly sample 5 of these numbers
sample(x = num_vec, size = 5)
#Reset random seed
set.seed(NULL)
```

That's right, `sample()` will always take the 10th (9/19), 6th (5/15), 5th (4/14), 4th (3/13), and 7th (6/16) values if it is given the __random seed__ of __1234__, provided with an 11 length vector, and asked to sample 5 values.

Setting our randomness is incredible beneficial for reproducibility in research.

When you carry out analysis you may need to redo some work. 
This could be due to reviewer comments or you want to incorporate some new methods.
As long as you saved the random seeds you used you can get the same results where you need to.
It also means others can replicate your results.

## Reset seed
<center>
![](figures/reset_seed.png){style="width:400px"}
</center>

We set a __random seed__ at the start of the cell for reproducibility and control, but why do we then run the line `set.seed(NULL)`?

The normal operation of R means that, in effect, its __random seed__ changes every time it is used.
This means R normally randomly determines randomness.
This is how it should be until we want to set the randomness.
It is therefore good practice to set the seed to `NULL` after you have utilised your set __seeds__.
This will revert the __seed__ to its normal random operations.

One last point to note is R versions. 
Version 3.6 changed R's sampling methods, therefore if you use Version 3.5 or below you will get different results than we have got.
Hopefully the R developers will not change this in a later version again.

## Random seed practice
<center>
![](figures/seed_sowing.png){style="width:200px"}
</center>

Brilliant! To reinforce the above knowledge try out the following challenges.

First create the following vector:

```{R, eval=FALSE}
second_millenium <- 1001:2000
```

__Note:__ Remember it is best practice to `set.seed(NULL)` at the end of a code cell.

### Random seed: Question 1

Sample the object `second_millenium` with the following parameters:

- Extract 10 values
- Without replacement
- Use the __random seed__ `489`

```{r, echo = FALSE}
opts_p <- c("__1120__", answer="__1369__", "__1744__")
```
What is the fourth number in the produced vector? `r longmcq(opts_p)`

### Random seed: Question 2

Sample the object `second_millenium` with the following parameters:

- Extract 24 values
- Without replacement
- Use the answer to the first question as the __random seed__

```{r, echo = FALSE}
opts_p <- c("__1120__", "__1369__", answer="__1744__")
```
What is the 16th number in the produced vector? `r longmcq(opts_p)`

### Random seed: Question 3

Sample the object `second_millenium` with the following parameters:

- Extract a number of values equal to the answer of the second question
- With replacement
- Use the answer to the first question as the __random seed__

```{r, echo = FALSE}
opts_p <- c(answer="__1120__", "__1369__", "__1744__")
```
What is the 999th number in the produced vector? `r longmcq(opts_p)`

Once you are happy you can save then close and halt your "Random_seeds.ipynb" notebook.

Hopefully this has given you a good understanding of the principle of __random seeds__. With this you can continue onto __iterative rarefaction__.

<!--chapter:end:07-Rarefaction_and_random_seeds.Rmd-->

```{r include=FALSE, cache=FALSE}
library(webexercises)
```
# Iterating rarefaction {#iterating_rarefaction_chap}
<center>
![](figures/france.png){style="width:200px"}
</center>

In this chapter we will create code to carry out __iterative rarefaction__.
For this we create a vector of __random seeds__, each used for a different __iteration__ of __rarefaction__.
We will loop through these __random seeds__, using each __random seed__ to carry out one __iteration__ of __rarefaction__.
In the next chapters we will utilise this code to produce alpha and beta diversity values that we will analyse.

## Iterating rarefaction dataset
<center>
![](figures/river.png){style="width:200px"}
</center>

We will utilise the same dataset used in the [R community analysis workshop](https://neof-workshops.github.io/R_community_whqkt8/Course/02-Dataset_and_workflow.html#dataset).

Below are brief bullet points about the data:

- It is a 16S dataset of ASV counts with taxonomy and phylogeny produced by QIIME2
- The samples come from surface water from the Durance River in the south-east of France
- There are three sampling sites on an anthropisation gradient (low to high agriculture)
  - Upper Durance (UD)
  - Middle Durance (MD)
  - Lower Durance (LD)
- Four different media approaches were used to produce bacterial lawns that were sequenced
  - Environmental sample (ENV): No media used, frozen at -20°C.
  - TSA 10% incubated at 28°C for 2 days.
  - KBC incubated at 28°C for 2 days.
  - CVP incubated at 28°C for 3 days.
- There are three replicates for each sampling site and media combination (36 samples total)

## Iterating rarefaction setup
<center>
![](figures/setup_5.png){style="width:200px"}
</center>

First, create a new R jupyter-notebook called "Iterating_rarefaction.ipynb".

At the top of this notebook create a code cell to load in the various packages and data we need. The code is below:

```{R, eval=FALSE}
#Libraries
library("phyloseq")
library("microbiome")
library("IRdisplay")
#Load processed but unrarefied data from R community analysis workshop
load("phyloseq.RData")
```

## Rarefaction iterations
<center>
![](figures/number_pad.png){style="width:200px"}
</center>

We need to choose the number of __iterations__ we are going to carry out.

For our __practice__ we will use __10 iterations__ for speed. 
In your real analysis I would recommend using __1000 iterations__.

Let's create a variable for our number of __iterations__.

```{R, eval=FALSE}
#Number of rarefaction iterations to be carried out
#Using 10 here for speed, real analysis should use 1000
rarefaction_iters <- 10
```

## RNG vector creation {#rng_vec_creation}
<center>
![](figures/list.png){style="width:200px; background-color:white; border-radius:15px"}
</center>

We can now carry out __Random Number Generation (RNG)__ to create a number of __random seeds__ equal to the number of iterations planned.

```{R, eval=FALSE}
#Create rngseed vector
#Set seed for reproducibility
#This number was chosen randomly
set.seed(2605)
#Sample 10 (number of iters) values from the number range 1-100,000
rngseed_vec <- sample(x=1:100000, size = rarefaction_iters, replace = FALSE)
#Print out vector
rngseed_vec
#Save our rngseed vector
save(rngseed_vec, file="rngseeds.RData")
#Reset seed
set.seed(NULL)
```

There are a lot of steps above. These are:

- __Setting the random seed:__ We carry this out so we will always get the same rngseed vector that will be used for the rarefaction iterations. This is important so you will always get the same results if you need to rework some analysis, stats, or plots. Also useful here so you get the same results as the instructor and other attendees.
- __Creating the rngseed vector:__ We use our old friend `sample()` to create a random number for each __iteration__ we will carry out. 
  - We arbitrarily sample from the numbers 1-100,000, you could change this to a larger range in your future research.
  - We use our previous object `rarefaction_iters` as `size=` to produce a random number for each of our __iterations__.
  - We carry this out __without replacement__ so none of our __rarefaction iterations__ are identical.
- __Save the rngseed vector:__ We save the vector as a file. We will load this in our alpha and beta diversity notebooks to be used for iterative rarefaction. This is also useful so you have a backup file of the rngseed vectors. 
- __Reset seed:__ Always good to reset the seed at the end of a cell.

## Phyla relative abundance

<center>
![](figures/river_2.png){style="width:200px"}
</center>

Prior to __iterative rarefaction__ we will look at the phyla composition of the environmental samples.

### Subset and phyla aggregation
<center>
![](figures/aggregate.png){style="width:200px"}
</center>

For demonstrative purposes we will reduce the amount of samples and features in our data for this chapter. We will carry this out by:

- Subsetting the data so it only contains the 9 environmental samples.
- Aggregate the taxa to phyla whilst aggregating rare taxa to one "other group"

```{R, eval=FALSE}
#Reduce data for demonstrative purposes
#Subset phyloseq object to only retain the ENV samples
#I.e. remove the media samples
physeq_env <- phyloseq::subset_samples(pseq, media == "ENV")

#Aggregate to phyla level whilst aggregating rare phyla
pseq_env_phyla <- microbiome::aggregate_rare(
  pseq_env, level = "Phylum",
  detection = 0.1, prevalence = 0.5,
  #Prevent info on aggregation to be printed out
  verbose = FALSE)
#View count table
otu_table(pseq_env_phyla)
#Sum count of samples
microbiome::readcount(pseq_env_phyla)
#Remove unwanted objects
rm(pseq, pseq_env)
```

### Phyla relative abundance bar chart
<center>
![](figures/stacked_bar_chart.png){style="width:200px"}
</center>

Let's have a quick look at the non rarefied phyla relative abundance through a bar chart.

__Note:__ This is how you would normally look at this type of bar chart.

```{R, eval=FALSE}
#Quick phyla bar chart of relative abundance
#Relative abundance transformation
pseq_env_phyla_relabund <- microbiome::transform(pseq_env_phyla, "compositional")
#Create, save, and display bar chart
phylum_bar <- microbiome::plot_composition(pseq_env_phyla_relabund)
ggsave(filename = "./env_phyla_relabund.png", plot = phylum_bar,
       device = "png", dpi = 300, units = "mm", height = 100, width = 100)
IRdisplay::display_png(file = "./env_phyla_relabund.png")
```

## One Round (1R) of rarefaction
<center>
![](figures/one_round.png){style="width:200px"}
</center>

We will first carry out one round of __rarefaction__ .
This is so we can get a reminder of how to carry it out and to compare the results of no __rarefaction__ and only one round.

### 1R: Rarefaction
<center>
![](figures/black_rhino.png){style="width:300px"}
</center>

Carry out one round of __rarefaction__ and view the __rarefied__ counts.

We are using the environmental samples subsetted and phyla aggregated data.
Additionally, we are using the first of our __random seeds__ in `rng_seed_vec` and the minimum read count as our __rarefaction__ size.

```{R, eval=FALSE}
#One round of rarefaction
pseq_env_phyla_rarefy_1 <- phyloseq::rarefy_even_depth(
  pseq_env_phyla,
  #Minimum read count as rarefaction size
  sample.size = min(microbiome::readcount(pseq_env_phyla)),
  #First random seed as the rng seed
  rngseed = rngseed_vec[1])
#View count table
otu_table(pseq_env_phyla_rarefy_1)
#Sum count of samples
microbiome::readcount(pseq_env_phyla_rarefy_1)
```

You should see that all the samples now have a total count of __11046__.

### 1R: Relative abundance bar chart
<center>
![](figures/stacked_bar_chart_2.png){style="width:200px"}
</center>

Now to create a relative abundance bar chart with our __rarefied__ data to compare to our non-rarefied data.

```{R, eval=FALSE}
#Quick phyla bar chart of relative abundance
#Relative abundance transformation
pseq_env_phyla_rarefy_1_relabund <- microbiome::transform(pseq_env_phyla_rarefy_1, "compositional")
#Create, save, and display bar chart
phylum_bar <- microbiome::plot_composition(pseq_env_phyla_rarefy_1_relabund)
ggsave(filename = "./env_phyla_rarefy_1_relabund.png", plot = phylum_bar,
       device = "png", dpi = 300, units = "mm", height = 100, width = 100)
IRdisplay::display_png(file = "./env_phyla_rarefy_1_relabund.png")
```

Viewing the non-rarefied and __rarefied__ based bar charts shows some differences. However, these are quite difficult to discern.

### 1R: Difference from non-rarefied
<center>
![](figures/histogram_1.png){style="width:300px"}
</center>

To more easily see the differences we will subtract the two relative abundance tables from each other.
This will produce a matrix of differences.

```{R, eval=FALSE}
#Value difference matrix
single_rarefaction_diff <- 
  phyloseq::otu_table(pseq_env_phyla_relabund) - phyloseq::otu_table(pseq_env_phyla_rarefy_1_relabund)
single_rarefaction_diff
```

We can see there are differences.
To make these differences even clearer let's make a histogram.

```{R, eval = FALSE}
#Histogram of differences
hist(single_rarefaction_diff, main = "Single rarefaction")
```

```{r, echo = FALSE}
opts_p <- c("__-0.0003(-3e-04) to 0.0003(3e-04)__", "__-0.003 to 0.003__", answer="__-0.015 to 0.015__")
```
What is the range of the differences compared to the non rarefied relative abundance values? `r longmcq(opts_p)`

Although these values appear quite small keep in mind we are working with relative abundance values.
Each sample has a total relative abundance of 1.00 so a relative abundance value of 0.01 is 1%.

Let's see if we can get these differences smaller with multiple rounds of rarefaction.

## Multiple Rarefaction (MR) iterations
<center>
![](figures/ten_rounds.png){style="width:200px"}
</center>

We will now carry out iterative rarefaction.

### MR: Rarefaction
<center>
![](figures/tiger.png){style="width:300px"}
</center>

The below loop creates a relative abundance table created by 10 rounds of iteration.
Type the code and read the annotations to understand what is going on.
Then run the code.

```{R, eval=FALSE}
#Iterative rarefaction to produce an average rarefied relative abundance table

#Assign rarefaction size
rarefaction_size <- min(microbiome::readcount(pseq_env_phyla))
#Read in our rng seed vector
load("rngseeds.RData")

#Initalise where we will store the output
#In this case we create the first iteration
#Carry out first rarefaction
pseq_rarefy <- phyloseq::rarefy_even_depth(pseq_env_phyla,
                                           sample.size = rarefaction_size
                                           #First random seed as the rng seed
                                           rngseed = rngseed_vec[1])
#Calculate relative abundance
pseq_rarefy_relabund <- microbiome::transform(pseq_rarefy, "compositional")
#Relabund phyla table object
relabund_phyla_table <- as.data.frame(phyloseq::otu_table(pseq_rarefy_relabund))

#Loop through the next 9 iterations
#Add the relabund rarefied values to phyla_table
for (i in 2:length(rngseed_vec)){
  #Rarefy
  pseq_rarefy <- phyloseq::rarefy_even_depth(pseq_env_phyla,
                                           sample.size = rarefaction_size
                                           rngseed = rngseed_vec[i])
  #Calculate relative abundance
  pseq_rarefy_relabund <- microbiome::transform(pseq_rarefy, "compositional")
  #Sum values to phyla_table
  relabund_phyla_table <- 
    relabund_phyla_table + as.data.frame(phyloseq::otu_table(pseq_rarefy_relabund))
}
#Average the values of the summed relabund phyla_table
relabund_phyla_table <- relabund_phyla_table / length(rngseed_vec)
```

### MR: Difference from non-rarefied
<center>
![](figures/histogram_3.png){style="width:200px"}
</center>

We'll skip the bar chart this time and only look at the difference of the values.

```{R, eval=FALSE}
#Value difference matrix
iterative_rarefaction_diff <- 
  as.matrix(phyloseq::otu_table(pseq_env_phyla_relabund) - relabund_phyla_table)
iterative_rarefaction_diff
#Histogram
hist(iterative_rarefaction_diff)
```

```{r, echo = FALSE}
opts_p <- c("__-0.0003(-3e-04) to 0.0003(3e-04)__", answer="__-0.003 to 0.003__", "__-0.015 to 0.015__")
```
What is the range of the differences compared to the non rarefied relative abundance values? `r longmcq(opts_p)`

You should notice that the differences are much smaller.
This indicates that the structure of the __iterative rarefied__ data is much closer to the non-rarefied data than the one __rarefied__ data.
This is what we want.

## Iterating rarefaction task
<center>
![](figures/per_thousand.png){style="width:200px"}
</center>

Superlative! Now that you know how to carry out __iterative rarefaction__ I'll ask you to do it once more for the phyla data.

Create a rarefaction averaged phyla relative abundance as we have done above but with 1000 __rarefaction iterations__.
For this task use __153478__ as the __seed__ when creating your vector of 1000 __rng seeds__.
Save this vector of rngseeds to a file called "rngseeds_1000.RData".

__Note:__ The iterative rarefaction step may take a few minutes. 

After creating the relative abundance matrix determine how different the values are compared to the non-rarefied relative abundance with a histogram.

```{r, echo = FALSE}
opts_p <- c(answer="__-0.0003(-3e-04) to 0.0003(3e-04)__", "__-0.003 to 0.003__", "__-0.015 to 0.015__")
```
What is the range of the differences compared to the non rarefied relative abundance values? `r longmcq(opts_p)`

Please attempt the task yourself before looking at the solution code in the below expandable box.

`r hide("Task solution code")`
```{R, eval=FALSE}
#Number of rarefaction iterations to be carried out
rarefaction_iters <- 1000
#Create rngseed vector
#Set seed for reproducibility
set.seed(153478)
#Create the rngseed vector
#Sample 1000 (number of iters) values from the number range 1-100,000
rngseed_vec <- sample(x=1:100000, size = rarefaction_iters, replace = FALSE)
#Save our rngseed vector
save(rngseed_vec, file="rngseeds_1000.RData")
#Reset seed
set.seed(NULL)
```

```{R, eval=FALSE}
#Iterative rarefaction to produce an average rarefied relative abundance table

#Assign rarefaction size
rarefaction_size <- min(microbiome::readcount(pseq_env_phyla))
#Read in our rng seed vector
load("rngseeds_1000.RData")

#Initalise where we will store the output
#In this case we create the first iteration
#Carry out first rarefaction
pseq_rarefy <- phyloseq::rarefy_even_depth(pseq_env_phyla,
                                           sample.size = rarefaction_size
                                           rngseed = rngseed_vec[1])
#Calculate relative abundance
pseq_rarefy_relabund <- microbiome::transform(pseq_rarefy, "compositional")
#Relabund phyla table object
relabund_phyla_table <- as.data.frame(phyloseq::otu_table(pseq_rarefy_relabund))

#Loop through the next 999 iterations
#Add the relabund rarefied values to phyla_table
for (i in 2:length(rngseed_vec)){
  #Rarefy
  pseq_rarefy <- phyloseq::rarefy_even_depth(pseq_env_phyla,
                                           sample.size = rarefaction_size
                                           rngseed = rngseed_vec[i])
  #Calculate relative abundance
  pseq_rarefy_relabund <- microbiome::transform(pseq_rarefy, "compositional")
  #Sum values to phyla_table
  relabund_phyla_table <- 
    relabund_phyla_table + as.data.frame(phyloseq::otu_table(pseq_rarefy_relabund))
}
#Average the values of the summed relabund phyla_table
relabund_phyla_table <- relabund_phyla_table / length(rngseed_vec)
```

```{R, eval=FALSE}
#Value difference matrix
iterative_rarefaction_1000_diff <- 
  as.matrix(phyloseq::otu_table(pseq_env_phyla_relabund) - relabund_phyla_table)
iterative_rarefaction_diff
#Histogram
hist(iterative_rarefaction_diff)
```
`r unhide()`

## Iterating rarefaction recap
<center>
![](figures/recap.png){style="width:200px"}
</center>

With this chapter you have learnt:

- How to create a vector of rngseeds
- How to use rng seeds to carry out __iterative rarefaction__

In you real life analysis you would not use this method to create relative abundance taxonomy bar charts, you would use the non-rarefied relative abundance.
However, this hopefully gave you a good idea of how __iterative rarefaction__ works so we can utilise it the next 2 chapters for alpha and beta diversity analysis.

Feel free to save then close and halt your "Iterating_rarefaction.ipynb" notebook.

<!--chapter:end:08-Iterating_rarefaction.Rmd-->

```{r include=FALSE, cache=FALSE}
library(webexercises)
```
# Alpha Diversity {#alpha_chap}
<center>
![](figures/alpha_iterative_rarefaction.png){style="width:300px"}
</center>

In this chapter we'll carry out __alpha diversity__ analysis using the __iterative rarefaction__ approach.
We will carry this out on the ASV counts rather than at a taxonomy level such as phyla.

These materials are mostly a combination of the __iterative rarefaction__ in this book and the __alpha diversity__ analysis in the R community workshop.
Due to this we won't go into great detail, instead focussing on giving you the code to be able to carry this out.

## 	$\alpha$: Setup
<center>
![](figures/setup_6.png){style="width:200px; background-color:white; border-radius:15px"}
</center>

Create a new R jupyter notebook called "Alpha_diversity.ipynb".

Load the required data and libraries.

```{R, eval = FALSE}
#Libraries
library("phyloseq")
library("microbiome")
library("tidyverse")
library("IRdisplay")
library("ggpubr")
#Load processed but unrarefied ASV data from main R community workshop
load("phyloseq.RData")
```

## $\alpha$: Iterative rarefaction values
<center>
![](figures/ten_rng_seeds.png){style="width:200px"}
</center>

Before carrying out __iterative rarefaction__ we need to decide on a few values

- __Rarefaction size:__ The sequence depth to normalise samples to
  - We are using the minimum sample depth here
  - The size you choose will be based on your data and what you feel is appropriate
  - More info in the R community workshop
- __RNG seeds:__ The rng seeds we will use for all the rarefactions
  - We created these in the previous [chapter](#rng_vec_creation)
- __Rarefaction iterations:__ The number of rarefaction iterations we will use
  - We are using 10 here based on the length of our rng seed vector
  - We recommend you use 1000 in your real analysis

```{r, eval=FALSE}
#Rarefaction values
#Rarefaction size
#Minimum sample depth in this case
rarefaction_size <- min(microbiome::readcount(pseq))
#Load the vector of 10 rngseeds created in the previous chapter
load("rngseeds.RData")
#Number of rarefaction iterations to be carried out
#Based on length of rng seed vector
rarefaction_iters <- length(rngseed_vec)
```

## $\alpha$: Iterative rarefaction loop
<center>
![](figures/iterative_loops.png){style="width:300px"}
</center>

Now we will create averaged __alpha diversity__ values through __iterative rarefaction__.
We will carry this out by:

- Calculating initial __alpha diversity__ values from the first __iteration__
- Looping through the subsequent __rarefaction iterations__ and adding/summing calculated __alpha diversity__ values to the initial __alpha diversity__ values
- Dividing each value of the final summed __alpha diversity__ by the number of __rarefaction iterations__.

For this we will use the function `microbiome::alpha()` to calculate our alpha diversity values.

```{r, eval=FALSE}
#Loop to create iteration based rarefied alpha diversity values

#Create data frame to contain final summed alpha diversity values
#In this case we'll run the first rarefied alpha diversity analysis
pseq_rarefy <- phyloseq::rarefy_even_depth(
  pseq,
  sample.size = rarefaction_size,
  rngseed = rngseed_vec[1],
  verbose = FALSE)
#Alpha diversity
alpha_df_sum <- microbiome::alpha(pseq_rarefy, index = "all")

#Loop through 2 to the number of iterations
for (i in 2:rarefaction_iters){
  pseq_rarefy <- phyloseq::rarefy_even_depth(
    pseq,
    sample.size = rarefaction_size,
    rngseed = rngseed_vec[i],
    verbose = FALSE)
  #Alpha diversity
  alpha_df <- microbiome::alpha(pseq_rarefy, index = "all")
  #Add/sum the new data frame values to the sum data frame
  alpha_df_sum <- alpha_df_sum + alpha_df
}
#Divide by number of rarefaction iterations to get average
alpha_df_mean <- alpha_df_sum / rarefaction_iters
#Save alpha mean data frame
save(alpha_df_mean, file = "alpha_df_mean.RData")
#Remove unneeded objects
rm(pseq,alpha_df_sum, alpha_df)
```

`r hide("verbose = FALSE option")`
We include the option `verbose = FALSE` in the `phyloseq::rarefy_even_depth()` to prevent a lot of text to be displayed.
This text says which rngseed was used in the rarefaction.
We don't need this message as we already have a record of the rngseeds we used in `rngseed_vec`.
`r unhide()`

You can check the structure and contents of our alpha diversity data frame with `head()`.

```{r, eval=FALSE}
head(alpha_df_mean)
```

There are a lot of diversity metrics.
We are only interested in a few of them here but we will remove the other ones later on.

## $\alpha$: Metric and metadata data frame
<center>
![](figures/wide_data_frame.png){style="width:200px"}
</center>

Now that we have our __alpha diversity__ values we will create a data frame that contains these values and metadata.

In the below code we extract the metadata from the rarefied phyloseq object created in the last loop iteration above.
This ensures we only acquire metadata from samples that are retained after __rarefaction__.
The samples that are retained are always the same across our __rarefaction iterations__ as samples are retained based on the __rarefaction size/depth__ which is kept consistent across our __rarefactions__.

```{r, eval=FALSE}
#Combine metadata and alpha diversity mean values into one data frame
#Extract metadata from rarefied phyloseq object
metadf <- phyloseq::sample_data(pseq)
#Ensure row names are identical
#if not sort alpha data frame rows by metadata row names
if (identical(row.names(metadf), row.names(alpha_df_mean)) == FALSE) {
  alpha_df_mean <- alpha_df_mean[row.names(metadf),]
}
#Combine with cbind (column bind)
meta_alpha_mean_df <- cbind(metadf,alpha_df_mean)
head(meta_alpha_mean_df)
#Remove rarefied phyloseq object that we do not need any more in this notebook
rm(pseq_rarefy) 
```

From the output of `head()` you should see a table with the first set of columns being the metadata columns. The next set of columns is the alpha diversity metrics.

## $\alpha$: Long data frame {#alpha_long_df}
<center>
![](figures/long_data_frame.png){style="width:100px"}
</center>

We will plot our __alpha diversity__ values with `ggplot2` functions.
Before we carry this out we need to convert our wide data frame to a long data frame.
We'll carry this out with `tidyr::pivot_longer()`.

For this we want our metric values to become long.
This means that instead of the __alpha diversity__ values being spread across multiple rows and columns their will only be one value per row.

Two columns will represent these values in the long format:

- __metric:__ The name of the alpha diversity metric
- __value:__ The value of the alpha diversity metric

```{r,eval=FALSE}
#Create long version for plotting
#alpha_df_mean (no metadata) column names to be used for long conversion
alpha_div_colnames <- colnames(alpha_df_mean)
#Wide to long
meta_alpha_mean_long_df <- 
  tidyr::pivot_longer(data = meta_alpha_mean_df,
                      #Change the alpha diversity names to long format
                      #I.e. keep our metadata columns as separate columns
                      #all_of() used to silence warning message
                      cols = all_of(alpha_div_colnames)
                      #Set column name to column called metric
                      #Set values to column called value
                      names_to = "metric", values_to = "value"
                      )
#Change our metric column to a factor
#Useful for plotting
meta_alpha_mean_long_df$metric <- as.factor(meta_alpha_mean_long_df$metric)
#Check head and dimensions of long data frame
head(meta_alpha_mean_long_df)
dim(meta_alpha_mean_long_df)
#Remove unneeded objects
rm(alpha_df_mean, metadf)
```

## $\alpha$: Subset metrics
<center>
![](figures/observe.png){style="width:200px; background-color:white; border-radius:15px"}
</center>

There are a lot of diversity values created by `microbiome::alpha()`.
For this tutorial we are only interested in:

- __observed:__ The number of observed features (ASVs, Phlya, Species, etc.)
- __chao1:__ The estimated real total number of features
- __diversity_shannon:__ The Shannon diversity metric
  - An estimate of feature diversity based on richness (presence/absence) and abundance
  - The higher the value the higher the diversity
  
We'll subset our long data frame to only retain rows with these metrics.
We'll also use the utility of factors to order the metrics so they are plotted in our preferred order further down.

```{R, eval=FALSE}
#Process our long data frame
#Subset our long alpha diversity data frame to only contain our metrics of choice
metrics <- c("observed", "chao1","diversity_shannon")
basic_alpha_metrics_long_df <- meta_alpha_mean_long_df[
  meta_alpha_mean_long_df$metric %in% metrics,
]
#Change instances of diversity_shannon to shannon
basic_alpha_metrics_long_df$metric <- gsub(pattern = "diversity_shannon",
                                           replacement = "shannon",
                                           x = basic_alpha_metrics_long_df$metric)
#The gsub() function changes our factor to a character vector
#Therefore change back to factor
#We will also choose our order of the metric names for plotting
basic_alpha_metrics_long_df$metric <- factor(x = basic_alpha_metrics_long_df$metric,
                                             levels = c("observed","chao1","shannon"))
#Check level order of metric factor column
levels(basic_alpha_metrics_long_df$metric)
#Check head of subsetted long data frame
head(basic_alpha_metrics_long_df)
```

With this data frame we can move onto visualisation.

## $\alpha$: Violin plot
<center>
![](figures/violin_2.png){style="width:100px"}
</center>

To visualise the differences of the alpha diversity values between the four different media we'll use violin plots.
We can use the function `ggplot2::geom_violin()` to carry this out.

Additionally, we'll add a point for each value and colour it based on the site it canme from (UD, MD, or LD).
The function `ggforce::geom_sina()` can be used for this.
We'll use its parameter `alpha()` to make the points 50% (`0.5`) transparent.

As we are plotting values from three different metrics we will split the plot into three separate plots.
`ggplot2::facet_wrap()` can be used for this tasked with `~metric` used to split the plot by the metrics.
We also specify `scales = "free"` so each of the three plots has their own x and y scales.
This is important when the values are drastically different such is the case between the observed and chao1 (>100) compared to the shannon values (<10).

`r hide("scales options")`
The four `scales` are:

- __`"fixed"` (default):__ All the scales are the same (fixed), based on the largest and smallest x and y values across all the plots. Useful where you want direct comparisons such as looking at teh overall pattern in ordination plots.
- __`"free"`:__ All the scales are free. Each plot's x and y values limits are based on the data within it.
- __`"free_x"`:__ The x axis is free and the y axis is fixed.
- __`"free_y"`:__ The y axis is free and the x axis is fixed.

Which you want to use depends on your data and how you are facetting it.
`r unhide()`

```{R, eval = FALSE}
#Produce ggplot object of violin plot
alpha_violinplot <- ggplot(basic_alpha_metrics_long_df, aes(x = media, y = value)) +
                            ggplot2::geom_violin() +
                            ggforce::geom_sina(alpha=0.5, aes(color=site)) +
                            ggplot2::labs(color = "Site", x = "Media", y = "Value") +
                            ggplot2::facet_wrap(~metric, scales = "free")
#Save ggplot2 object with ggsave()
ggsave(filename = "./Alpha_diversity_rarefy_iters_media_violinplot.png", plot = alpha_violinplot,
       device = "png", dpi = 300, units = "mm", height = 150, width = 250)
#Display plot
IRdisplay::display_png(file = "./Alpha_diversity_rarefy_iters_media_violinplot.png")
```

## $\alpha$: Stats
<center>
![](figures/stats.png){style="width:200px"}
</center>

We can carry out statistics to compare the alpha diversity values between sample groups.

### Kruskal Wallis test

To determine if there is an overall difference in our data we'll use the Kruskal Wallis test.
We'll carry this out using the media grouping for our three alpha diversity values.

```{R, eval =FALSE}
#Kruskal Wallis test
#Observed ASVs
kruskal.test(observed ~ media, data = meta_alpha_mean_df)
#Chao1 estimator
kruskal.test(chao1 ~ media, data = meta_alpha_mean_df)
#Shannon diversity
kruskal.test(shannon ~ media, data = meta_alpha_mean_df)
```

All the p-values are less than 0.05 indicating statistical significance.
That means we can move onto pairwise comparisons.

### Paired wilcoxon test

To determine what groups are significantly different from each other we can carry out paired Wilcoxon test.
This tests 

```{R, eval=FALSE}
#Paired wilcoxon test
#Observed ASVs
pairwise.wilcox.test(meta_alpha_mean_df$observed, meta_alpha_mean_df, p.adjust.method = "holm")
#Chao1 estimator
pairwise.wilcox.test(meta_alpha_mean_df$chao1, meta_alpha_mean_df, p.adjust.method = "holm")
#Shannon diversity
pairwise.wilcox.test(meta_alpha_mean_df$shannon, meta_alpha_mean_df, p.adjust.method = "holm")
```

You'll see three p-value adjusted tables with all the values (except Shannon: ENV against TSA) being significant (<0.05).

## $\alpha$: Plot with stats
<center>
![](figures/violin_plot_w_stats.png){style="width:200px; background-color:white; border-radius:15px"}
</center>

Rather than having the plot and stats separate, we can add stats onto our plot.
This can be carried out with the function `stat_compare_means()` from the ``ggpubr` package.

### List of comparisons
<center>
![](figures/venn_pairwise.png){style="width:200px; background-color:white; border-radius:15px"}
</center>

To produce pairwise comparisons with `ggpubr::stat_compare_means()` we need a __list__ of the comparisons we want to carry out.

We can create this with the function `combn()`, short for combination.
We provide it with three parameters:

- __Input data:__ This is a __vector__ of the unique metadata categories to create the combinations from
  - We are using our created `uniq_media_values_chr_vec` in this case
  - We ensure that this is a __vector__ of __characters__ so the created combination __list__ contains __character vectors__
  - A __list__ of __character vectors__ is required for `ggpubr::stat_compare_means()`
- __`m = `:__ The number of elements to choose when creating combinations.
  - We choose `2` so we get all pair combinations
- __`simplify = `:__ Indicates if the result should be simple (`TRUE`) or not (`FALSE`)
  - `TRUE` returns a simplified array such as a __matrix__ or a __data frame__
  - `FALSE` returns a __list__. This is what we want as `ggpubr::stat_compare_means()` requires a __list__

```{R,eval = FALSE}
#To compare mean we need to create a list of comparisons
#Create character vector of unique metadata values (media in this case)
uniq_media_values_chr_vec <- unique(as.character(basic_alpha_metrics_long_df$media))
uniq_media_values_chr_vec
#Can use combn() to get comparisons
my_comparisons <- combn(uniq_media_values_chr_vec, m = 2, simplify = FALSE))
#Check contents and structure
my_comparisons
str(my_comparisons)
```

### Violin plot with stats
<center>
![](figures/eye_2.png){style="width:200px"}
</center>

With our __list__ of comparisons we can add `ggpubr::stat_compare_means()` to our `ggplot2` code.
This function will both calculate the Wilcoxon tests and add them to the plot.

```{R, eval = FALSE}
#Produce ggplot object of violin plot
alpha_violinplot <- ggplot(basic_alpha_metrics_long_df, aes(x = media, y = value)) +
                            ggplot2::geom_violin() +
                            ggforce::geom_sina(alpha=0.5, aes(color=site)) +
                            ggplot2::labs(color = "Site", x = "Media", y = "Value") +
                            ggplot2::facet_wrap(~metric, scales = "free") +
                            #Add comparisons
                            ggpubr::stat_compare_means(comparisons = my_comparisons)
#Save ggplot2 object with ggsave()
ggsave(filename = "./Alpha_diversity_rarefy_iters_media_violinplot_pairwise_wilcox.png", 
       plot = alpha_violinplot,
       device = "png", dpi = 300, units = "mm", height = 150, width = 250)
#Display plot
IRdisplay::display_png(file = "./Alpha_diversity_rarefy_iters_media_violinplot_pairwise_wilcox.png")
```

### Reorder x-axis and stats
<center>
![](figures/reorder.png){style="width:200px; background-color:white; border-radius:15px"}
</center>

`ggplot` orders the x-axis by alphabetical order.
This is not normally wanted so we will convert our media column to a __factor__ and order the levels how we want them.
As the environmental samples can be seen as the baseline we will have them first.

```{r,eval = FALSE}
#Set order of media
basic_alpha_metrics_long_df$media <- factor(basic_alpha_metrics_long_df$media,
                                            #Set order of levels
                                            levels = c("ENV", "CVP", "KBC", "TSA"))
```

The stats in our previous plot were also not in a good order.
We'll therefore reorder them.
When doing this it is important to note that the first comparison in the __lists__ is the bottom most stat in the plot.

```{R, eval = FALSE}
#Order comparisons
my_ordered_comparisons <- my_comparisons[c(1,2,6,4,3,5)]
my_ordered_comparisons
```

You'll notice this can be quite manual.
It can be made easier when doing this yourself to roughly reorder, run the below code for your plot, then fix the stats reorder.

With our media categories and comparisons reordered we can create the final plot.

```{R, eval = FALSE}
#Produce ggplot object of violin plot
alpha_violinplot <- ggplot(basic_alpha_metrics_long_df, aes(x = media, y = value)) +
                            ggplot2::geom_violin() +
                            ggforce::geom_sina(alpha=0.5, aes(color=site)) +
                            ggplot2::labs(color = "Site", x = "Media", y = "Value") +
                            ggplot2::facet_wrap(~metric, scales = "free") +
                            #Add comparisons
                            ggpubr::stat_compare_means(comparisons = my_ordered_comparisons)
#Save ggplot2 object with ggsave()
ggsave(filename = "./Alpha_diversity_rarefy_iters_media_violinplot_pairwise_wilcox.png", 
       plot = alpha_violinplot,
       device = "png", dpi = 300, units = "mm", height = 150, width = 250)
#Display plot
IRdisplay::display_png(file = "./Alpha_diversity_rarefy_iters_media_violinplot_pairwise_wilcox.png")
```

## $\alpha$: Task
<center>
![](figures/tasks.png){style="width:200px; background-color:white; border-radius:15px"}
</center>

As an optional task if you want more practice create a new violin plot that includes the following:

- Plots the metrics:
  - "chao1"
  - "evenness_pielou" renaming it as "pielou"
  - "diversity_fisher" renaming it as "fisher"
  - Ensure they are in the order "chao1", "pielou", then "fisher"
- The x-axis is separated by Site (UD, MD, LD) rather than media.
  - Ensure the order is UD, MD, then LD
- Points are coloured by Media (ENV, CVP, KBC, and TSA)
  - Ensure the order is ENV, CVP, KBC, then TSA
- Include Wilcoxon paired stats comparing the Sites

`r hide("Solution code")`
Subset [long alpha diversity table created earlier in this chapter](#alpha_long_df) to contain metrics of choice.
```{R, eval=FALSE}
#Process our long data frame
#Subset our long alpha diversity data frame to only contain our metrics of choice
metrics <- c("chao1", "evenness_pielou","diversity_fisher")
subset_alpha_metrics_long_df <- meta_alpha_mean_long_df[
  meta_alpha_mean_long_df$metric %in% metrics,
]
#Change instances of evenness_pielou to pielou
subset_alpha_metrics_long_df$metric <- gsub(pattern = "evenness_pieloue",
                                           replacement = "pielou",
                                           x = subset_alpha_metrics_long_df$metric)
#Change instances of diversity_fisher to fisher
subset_alpha_metrics_long_df$metric <- gsub(pattern = "diversity_fisher",
                                           replacement = "fisher",
                                           x = subset_alpha_metrics_long_df$metric)
#The gsub() function changes our factor to a character vector
#Therefore change back to factor
#We will also choose our order of the metric names for plotting
subset_alpha_metrics_long_df$metric <- factor(x = subset_alpha_metrics_long_df$metric,
                                             levels = c("chao1","pielou","fisher"))
#Check level order of metric factor column
levels(subset_alpha_metrics_long_df$metric)
#Check head of subsetted long data frame
head(subset_alpha_metrics_long_df)
```

Create metadata combination list for plot stats
```{R,eval = FALSE}
#To compare mean we need to create a list of comparisons
#Create character vector of unique metadata values (site in this case)
uniq_site_values_chr_vec <- unique(as.character(subset_alpha_metrics_long_df$site))
uniq_site_values_chr_vec
#Can use combn() to get comparisons
my_comparisons <- combn(uniq_site_values_chr_vec, m = 2, simplify = FALSE))
#Check contents and structure
my_comparisons
str(my_comparisons)
```

Reorder factors and comparisons
```{r,eval = FALSE}
#Reorder sites, media and comparisons
#Set order of sites
subset_alpha_metrics_long_df$site <- factor(subset_alpha_metrics_long_df$site,
                                            #Set order of levels
                                            levels = c("UD", "MD", "LD"))
#Set order of media
subset_alpha_metrics_long_df$media <- factor(subset_alpha_metrics_long_df$media,
                                            #Set order of levels
                                            levels = c("ENV", "CVP", "KBC", "TSA"))
#Order comparisons
my_ordered_comparisons <- my_comparisons[c(1,3,2)]
my_ordered_comparisons
```

Plot with stats
```{R, eval = FALSE}
#Produce ggplot object of violin plot
alpha_violinplot <- ggplot(subset_alpha_metrics_long_df, aes(x = site, y = value)) +
                            ggplot2::geom_violin() +
                            ggforce::geom_sina(alpha=0.5, aes(color=site)) +
                            ggplot2::labs(color = "Media", x = "Site", y = "Value") +
                            ggplot2::facet_wrap(~metric, scales = "free") +
                            #Add comparisons
                            ggpubr::stat_compare_means(comparisons = my_ordered_comparisons)
#Save ggplot2 object with ggsave()
ggsave(filename = "./Alpha_diversity_rarefy_iters_site_violinplot_pairwise_wilcox.png", 
       plot = alpha_violinplot,
       device = "png", dpi = 300, units = "mm", height = 150, width = 250)
#Display plot
IRdisplay::display_png(file = "./Alpha_diversity_rarefy_iters_site_violinplot_pairwise_wilcox.png")
```
`r unhide()`

## $\alpha$: Recap
<center>
![](figures/recap.png){style="width:200px; background-color:white; border-radius:15px"}
</center>

In this chapter you have:

- Produced __alpha diversity__ values through __iterative rarefaction__
- Created a long data frame containing metadata and specified __alpha diversity__ metrics
- Visualised the group differences of __alpha diversity__ metrics with violin plots
- Embedded paired Wilcoxon p-values in our violin plots

With these skills and knowledge you will be able to carry out thorough investigations of __alpha diversity__ in your future research.

<!--chapter:end:09-Alpha_diversity.Rmd-->

```{r include=FALSE, cache=FALSE}
library(webexercises)
```
# Beta Diversity {#beta_chap}
<center>
![](figures/beta_iterative_rarefaction.png){style="width:300px"}
</center>

This chapter will teach you how to carry out __beta diversity__ analysis.
The steps will include:

- Calculating __beta diversity__ distances
- Using __iterative rarefaction__ to create averaged distance values
- Carrying various ordination techniques to visualise the distances between samples

## $\beta$: Setup
<center>
![](figures/setup_7.png){style="width:200px"}
</center>

Create a new R jupyter notebook called "Beta_diversity.ipynb".

Load the required data and libraries.

```{R, eval = FALSE}
#Libraries
library("phyloseq")
library("microbiome")
library("IRdisplay")
library("vegan")
library("rbiom")
library("ape")
#Load processed but unrarefied ASV data from main R community workshop
load("phyloseq.RData")
```

## $\beta$: Distance matrices
<center>
![](figures/measuring_tape_2.png){style="width:300px"}
</center>

Here we'll find out how to produce a paired distance matrix with our __beta diversity__ metric of choice.
Unfortunately there is not one function that can calculate all the metrics we may want.

### Unifrac distances
<center>
![](figures/weighted_blanket.png){style="width:200px"}
</center>

Unifrac distances are preferred by many when a phylogenetic tree is available for your data.
We are working with 16S data so a phylogenetic tree was created.
Other barcodes or types of data may not have a phylogenetic such as ITS data or complex shotgun metagenomic data.

To produce a Unifrac distance matrix we will use the function `unifrac()` from the package `rbiom`.

The function `rbiom::unifrac()` was created to work with biom files and not __phyloseq__ objects.
Thankfully we don't need to convert our objects to a biom object, instead only needing to extract our count/abundance data and phylogenetic tree.
We can carry this out with the following 2 functions:

- `phyloseq::otu_table()`: Extracts the feature count/abundances table
- `phyloseq::phy_tree()`: Extract the phylogenetic tree

There are 2 types of Unifrac distances:

- Weighted: Incorporate relative abundances
- Unweighted: Does not incorporate relative abundances

We'll calculate weighted Unifrac distances by specifying the parameter `weighted = TRUE`.

So our output works with subsequent ordination step we'll convert the output of `rbiom::unifrac()` into a matrix with `as.matrix()`.

Carry out the matrix production with the below script:

```{R, eval=FALSE}
#Calculate weighted unifrac values
unifrac_rbiom_microbiome <- as.matrix(
  rbiom::unifrac(biom = phyloseq::otu_table(pseq),
                 tree = phyloseq::phy_tree(pseq),
                 weighted = TRUE))
#Check the first 6 rows and columns of the resulting distance matrix
head(unifrac_rbiom_microbiome)
```

### Vegan distances
<center>
![](figures/vegan_fork.png){style="width:200px"}
</center>

To calculate non-unifrac __beta diversity__ distances we can use the `vegan` package and its function `vegdist()`.

The function `rbiom::unifrac()` calculates paired distances, this is the samples in the `otu_table` of a phyloseq object.
Unfortunately, `vegan::vegdist()` calculates paired distances by rows (features), this being the features in the `otu_table` of a phyloseq object.
We do not want this but thankfully all we need to add is the function `t()` to our abundance object to transpose the data.

Create a __Bray-Curtis__ distance matrix with the below code:

```{R, eval=FALSE}
#Calculate Bray-Curtis distance matrix
bray_curtis_mat <- as.matrix(
  vegan::vegdist(x = t(phyloseq::otu_table(pseq)),
                 method = "bray"))
#Check first 6 rows and columns of matrix
bray_curtis_mat[1:6,1:6]
```

The function can calculate a plethora of metrics with the full list available at the following [webpage](https://rdrr.io/cran/vegan/man/vegdist.html).

## $\beta$: Iterative rarefaction
<center>
![](figures/iterative_distances.png){style="width:400px"}
</center>

Now that we know how to create a __beta diveristy__ paired distance matrix, we can create one with averaged values created by __iterative rarefaction__.

### Iterative rarefaction values
<center>
![](figures/ten_rng_seeds.png){style="width:200px"}
</center>

We need to set our __rarefaction__ values and rngseeds.
We can use the same code as we used in the __alpha_diversity__ analysis.

```{R, eval = FALSE}
#Rarefaction values
#Rarefaction size
#Minimum sample depth in this case
rarefaction_size <- min(microbiome::readcount(pseq))
#Load the vector of 10 rngseeds created in the previous chapter
load("rngseeds.RData")
#Number of rarefaction iterations to be carried out
#Based on length of rng seed vector
rarefaction_iters <- length(rngseed_vec)
```

### Iterative beta diversity calculation
<center>
![](figures/mean.png){style="width:200px"}
</center>

The below code carries out __itertaive rarefaction__ and produces an averaged __weighted unifrac__ paired distance matrix.

```{R, eval = FALSE}
#Loop to create iteration based rarefied weighted unifrac values

#Create matrix to contain summed wunifrac beta diversity values
#In this case we'll run the first rarefied beta diversity analysis
pseq_rarefy <- phyloseq::rarefy_even_depth(
  pseq,
  sample.size = rarefaction_size,
  rngseed = rngseed_vec[1],
  verbose = FALSE)
#wunifrac beta diversity
beta_df_sum <- as.matrix(
  rbiom::unifrac(
    biom = phyloseq::otu_table(pseq_rarefy),
    tree = phyloseq::phy_tree(pseq_rarefy),
    weighted = TRUE))

#Loop through 2 to the number of iterations
for (i in 2:rarefaction_iters){
  #Rarefaction
  pseq_rarefy <- phyloseq::rarefy_even_depth(
    pseq,
    sample.size = rarefaction_size,
    rngseed = rngseed_vec[i],
    verbose = FALSE)
  #Beta diversity
  beta_df <- as.matrix(
    rbiom::unifrac(
      biom = phyloseq::otu_table(pseq_rarefy),
      tree = phyloseq::phy_tree(pseq_rarefy),
      weighted = TRUE))
  #Add/sum the new data frame values to the sum data frame
  beta_df_sum <- beta_df_sum + beta_df
}
#Divide by number of rarefaction iterations to get average
beta_df_mean <- beta_df_sum / rarefaction_iters
#Save alpha mean data frame
save(beta_df_mean, file = "wunifrac_df_mean.RData")
#View first 6 rows and columns of matrix
beta_df_mean[1:6,1:6]
#Remove unneeded objects
rm(beta_df_sum, beta_df_mean, pseq_rarefy)
```

`r hide("verbose = FALSE option")`
We include the option `verbose = FALSE` in the `phyloseq::rarefy_even_depth()` to prevent a lot of text to be displayed.
This text says which rngseed was used in the rarefaction.
We don't need this message as we already have a record of the rngseeds we used in `rngseed_vec`.
`r unhide()`

You'll notice that each value is duplicated, once in the bottom left triangle and once in the top right triangle.
This is fine as the subsequent functions in this chapter will work with this in the same manner as if the matrix was de-duplicated.

In the above case we saved our final distance matrix and removed it.
We will then load the object in the next section.
This is convenient as it means we only need to run this code cell once. 
With higher numbers of rarefaction iterations it can take a while.

## $\beta$: Ordination
<center>
![](figures/navigation.png){style="width:200px"}
</center>

There are various ways to ordinate paired dissimilarity distances.
We are going to use two of the most popular for community based data: __NMDS__ and __PCoA__.

### Load weighted unifrac matrix
<center>
![](figures/loading.png){style="width:300px"}
</center>

Prior to ordination we need to load the weighted unifrac matrix.
This is a useful cell to have as we don't need to rerun the __iterative rarefaction__ to reacquire this object if we save, close + halt, and then reopen this notebook.

```{R, eval=FALSE}
#Load wunifrac matrix
load("wunifrac_df_mean.RData")
```

### NMDS






### Which ordination method to use?

<!--chapter:end:10-Beta_diversity.Rmd-->

